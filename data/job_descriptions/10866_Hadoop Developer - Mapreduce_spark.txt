TITLE: Hadoop Developer - Mapreduce/spark

SKILLS: IT Software - DBA

Job Description Â Send me Jobs like this 1. Spark OR Storm: Candidates should be able to design systems and have experience working with these parts of the Hadoop ecosystem in a production environment (meaning REAL application development). They should have experience with scalability. 2.Experience with Hortonworks OR Cloudera 3. Java Financial services are not preferred. - Team Size: 4 - Location: New York (277 Park) - Duration: 6 month contract to hire/This is a 2 year program Qualifying Questions: - What part of Hadoop should they focus on- (pig, hive, MapReduce, Cassandra, Splunk, storm) The manager told us that the immediate need would be on the WCAS project. This project would require someone with either Spark or Storm. The SCPP project would require someone with Cassandra. He is open to seeing either at this time. - If they don- t come from a java background, is Python OK- Is Java necessary- They do not need to come from a java background. They need to have an understanding of Hadoop with a deeper understanding of Cassandra, Spark or Storm. - Any other scripting experience- No - Is the ecosystem on Hortonworks or Cloudera- Which do they need- They use both based on the project so either is fine. - WCAS - WCAS is a project related to risk and regulatory reporting. JPMC currently has to have a certain amount of capital should the bank - go under.- However, in order for this information to be correct, they have to have accurate historical data and risk models. This project using Big Data technology to create and update risk models to provide regulators. The goal is to refine the data and free up capital. WCAS - Hive - SCPP - Securities Core Processing Platform. This project is related to a reengineering of the JPMC custody platform for Fortress. Each application is currently developed as a silo and they are reengineer this within the accounts receivable space. It is important that they find candidates that have experience with scalability as this application supports 25,000 trades per second. - This is a 2 year program ; Use amp and Cassandra - App: 20% functional; 80% technical (non-functional) This is related to the importance of the application not relying on the usability but actually working. Big Data Applications Developer with a deep knowledge of Spark, Map Reduce and the Hadoop Ecosystem. This is a development role. As part of the team the candidate will work build cutting edge systems. We are looking for a person that wants to contribute to a robust team of developers, and continuously deepen the big data skills. Salary: Not Disclosed by Recruiter Industry: IT-Software / Software Services Functional Area: IT Software - DBA , Datawarehousing Role Category:Programming & Design Role:Software Developer Keyskills Hadoop Pig Java Accounting Python Finance Scalability Application Development Regulatory Reporting Financial Services Desired Candidate Profile Education- UG: Any Graduate - Any Specialization PG:Any Postgraduate - Any Specialization Doctorate:Any Doctorate - Any Specialization, Doctorate Not Required - Expertise with Hadoop and the HDFS and Ecosystem- Experience developing optimum strategies for distributing data over a cluster and distributed applications- Strong development skills on Map Reduce and Spark applications- Strong Experience with data management and partitioning- Experience in troubleshooting, debugging and optimizing Map Reduce- Experience working with Hortonworks or Cloudera (preferred)- Experience with batch processing- Experience with streaming processing- Experience in streams processing using Spark Streaming / Storm- Experience with Impala is a big plus- Experience with Core Java, Scala, Python, and R- Experience integrating a relational database and a NoSQL store and SQL application development experience- Exposure to ETL tools such as data torrent and Pentaho is a plus- Experience with Navigator is a plus- Experience with REST API is a plus- Exposure to encryption tools (HP Voltage) is a plusRole :- Contribute to application design- Be one of the developer on the Hadoop platform- Work with peers in the team, and in the bigger group as part of code reviews and design sessions- Work with technical and business partners to understand needs and provide innovative solutions- Document and test codeJPMIS acts as the center of excellence for big data projects at JPMorgan Chase. The work environment can be high pressured at times due to deadline intensive projects. In addition, most resources will need to feel comfortable interacting with the business since this group consults with other lines of business. Although, JPMC is an investment bank, many groups understand they need to attract talent which may not want to work in a "stuffy" environment. Because of this challenge, the group is opening up a "JPMC West" office which will have a more start-up feel.Exposure to big data technology. Fully funded by 2nd largest bank in the world, as opposed to a startup. Currently the #1 bank in the world.1.JPMIS is making a difference. They are applying their expertise to help make better decisions and protect all of us and our financial health.2.JPMIS lacks politics. Being in one of the largest financial institutions, it is a common assumption that the organization would be mired down in politics. This is far from the case within JPMIS. They are an agile, collaborative environment that is focused and encourages everyone to make a difference3.By working at JPMIS, our consultants will have increased marketability. Not only will team members make an impact, they will be able to carry away franchise value into their future endeavors.- Excellent customer service attitude, communication skills (written and verbal), and interpersonal skills.- Experience working in cross-functional, multi-location teams.- Excellent analytical and problem-solving skills. Company Profile: Softtech Career Infosystem Pvt. Ltd Leading client of Softtech Career Infosystem Pvt. Ltd Download PPT Photo 1 Â View Contact Details